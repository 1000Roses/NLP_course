{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_bai1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1nn6Pz0N_zfbAy2p6iGuv5FBVUf9_qxzw",
      "authorship_tag": "ABX9TyOxtShypIswoFkhvxnUsiC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1000Roses/NLP_course/blob/main/final_bai1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uz7n372_a0n"
      },
      "source": [
        "# Install lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri1HKPac_iDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be019b77-84d0-4b64-f4bb-a598d23afa72"
      },
      "source": [
        "!pip install underthesea\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers<=3.5.1,>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.5.1)\n",
            "Requirement already satisfied: torch<=1.5.1,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.5.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.9.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.0.45)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDXevGtW_mKE"
      },
      "source": [
        "# Import lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2tWRDvbX8Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3a4c41-57d7-4a68-daab-cb3aa16b8e31"
      },
      "source": [
        "from underthesea import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For Bert\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# For read file\n",
        "import glob\n",
        "print(glob.glob(\"/content/drive/MyDrive/NLP/chatbotdata/*.txt\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/NLP/chatbotdata/giải trí.txt', '/content/drive/MyDrive/NLP/chatbotdata/du lịch.txt', '/content/drive/MyDrive/NLP/chatbotdata/shoping.txt', '/content/drive/MyDrive/NLP/chatbotdata/gia đình.txt', '/content/drive/MyDrive/NLP/chatbotdata/tán gẫu.txt', '/content/drive/MyDrive/NLP/chatbotdata/các câu hỏi phức tạp.txt', '/content/drive/MyDrive/NLP/chatbotdata/đất nước.txt', '/content/drive/MyDrive/NLP/chatbotdata/nghỉ lễ.txt', '/content/drive/MyDrive/NLP/chatbotdata/học tập.txt', '/content/drive/MyDrive/NLP/chatbotdata/thông tin cá nhân.txt', '/content/drive/MyDrive/NLP/chatbotdata/sở thích.txt', '/content/drive/MyDrive/NLP/chatbotdata/trò chuyện về đi ăn.txt', '/content/drive/MyDrive/NLP/chatbotdata/robot.txt', '/content/drive/MyDrive/NLP/chatbotdata/địa chỉ.txt', '/content/drive/MyDrive/NLP/chatbotdata/người yêu.txt', '/content/drive/MyDrive/NLP/chatbotdata/tdtu.txt', '/content/drive/MyDrive/NLP/chatbotdata/bạn bè.txt', '/content/drive/MyDrive/NLP/chatbotdata/nghề nghiệp.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHkxOCJk_zUM"
      },
      "source": [
        "# Read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk468adXJnYK"
      },
      "source": [
        "all_path = glob.glob(\"/content/drive/MyDrive/NLP/chatbotdata/*.txt\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEDJHPAxX_y3"
      },
      "source": [
        "FRIEND_PRE_DATA = list()\n",
        "VOCAB_SIZE = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng3dd1wQYGP2"
      },
      "source": [
        "for path in all_path:\n",
        "  with open(path) as frienddata:\n",
        "    for data in frienddata:\n",
        "      sent = data.split(\"__eou__\")[:2]\n",
        "      if sent[0] == ' ' or sent[1] == ' ':\n",
        "        continue\n",
        "      if len(sent) == 2:\n",
        "        anq = list()\n",
        "        for subsent in sent:\n",
        "          anq.append(subsent)\n",
        "        FRIEND_PRE_DATA.append(anq)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Zse5mnZWZW"
      },
      "source": [
        "df = pd.DataFrame(FRIEND_PRE_DATA, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9VZPFDZgyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "7607a7df-0731-4f7c-bb88-ee8abccf854b"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim Marvel nhất</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ Noo Phước Thịnh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                            Answer\n",
              "0                    bạn thích hãng phim nào nhất ?                  mình thích hãng phim Marvel nhất \n",
              "1                   thể loại phim bạn thích là gì ?    mình thích thể loại phim hành động, viễn tưởng \n",
              "2               bạn có thần tượng ca sĩ nào không ?              mình rất thích ca sĩ Noo Phước Thịnh \n",
              "3        bạn có muốn thử vận may vào việc chơi đề ?                            mình không muốn cờ bạc \n",
              "4           tết này bạn có muốn đi xem phim không ?                      tất nhiên rồi, mình rất muốn \n",
              "5              cuối tuần này bạn có rãnh hay không?                           cuối tuần này mình rãnh \n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...                             dĩ nhiên là muốn rồi \n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...                 xin lỗi mai mình có việc bận rồi \n",
              "8   bạn có dự định đi chơi đâu vào cuối tuần không?            không hiện tại mình chưa có dự định gì \n",
              "9  bạn cho mình hỏi bạn thích xem phim thể loại gì?              tớ thích xem phim thể loại hành động "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuHBeU9stma"
      },
      "source": [
        "document_token = [ \" \".join([ e.replace(' ','_') for e in word_tokenize(sent.lower())]) for sent in df['Question']]\n",
        "document_token_list = [word_tokenize(sent.lower()) for sent in df['Question']]  \n",
        "question_lower = [question.lower() for question in df['Question']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VxGzbAhsuU5"
      },
      "source": [
        "df[\"Word_segmentation\"] = document_token\n",
        "df[\"Tokenize_list\"] = document_token_list\n",
        "df['Question'] = question_lower"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vBIXGRrsv8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "996a12e9-1a0c-44b9-9c84-c4d4ffad47ee"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Word_segmentation</th>\n",
              "      <th>Tokenize_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim Marvel nhất</td>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>[bạn, thích, hãng, phim, nào, nhất, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "      <td>thể_loại phim bạn thích là gì ?</td>\n",
              "      <td>[thể loại, phim, bạn, thích, là, gì, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ Noo Phước Thịnh</td>\n",
              "      <td>bạn có thần_tượng ca_sĩ nào không ?</td>\n",
              "      <td>[bạn, có, thần tượng, ca sĩ, nào, không, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>[bạn, có, muốn, thử, vận, may, vào, việc, chơi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>[tết, này, bạn, có, muốn, đi, xem, phim, không...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "      <td>cuối tuần này bạn có rãnh hay không ?</td>\n",
              "      <td>[cuối, tuần, này, bạn, có, rãnh, hay, không, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>[bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi_mình ...</td>\n",
              "      <td>[mai, bạn, có, muốn, đi, đá, banh, chung, với,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "      <td>bạn có dự_định đi chơi đâu vào cuối tuần không ?</td>\n",
              "      <td>[bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể_loại gì ?</td>\n",
              "      <td>[bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  ...                                      Tokenize_list\n",
              "0                    bạn thích hãng phim nào nhất ?   ...             [bạn, thích, hãng, phim, nào, nhất, ?]\n",
              "1                   thể loại phim bạn thích là gì ?   ...            [thể loại, phim, bạn, thích, là, gì, ?]\n",
              "2               bạn có thần tượng ca sĩ nào không ?   ...        [bạn, có, thần tượng, ca sĩ, nào, không, ?]\n",
              "3        bạn có muốn thử vận may vào việc chơi đề ?   ...  [bạn, có, muốn, thử, vận, may, vào, việc, chơi...\n",
              "4           tết này bạn có muốn đi xem phim không ?   ...  [tết, này, bạn, có, muốn, đi, xem, phim, không...\n",
              "5              cuối tuần này bạn có rãnh hay không?   ...    [cuối, tuần, này, bạn, có, rãnh, hay, không, ?]\n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...  ...  [bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...\n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...  ...  [mai, bạn, có, muốn, đi, đá, banh, chung, với,...\n",
              "8   bạn có dự định đi chơi đâu vào cuối tuần không?   ...  [bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...\n",
              "9  bạn cho mình hỏi bạn thích xem phim thể loại gì?   ...  [bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZfhVas-bOPU"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p0LQ2QUsTpV"
      },
      "source": [
        "countVectorizer_sent = CountVectorizer()\n",
        "countVectorizer_corpus = CountVectorizer()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIiYa8kxa9nT"
      },
      "source": [
        "model_countVectorizer_sent = countVectorizer_sent.fit(df['Word_segmentation'])\n",
        "model_countVectorizer_corpus = countVectorizer_sent.fit_transform(df['Word_segmentation'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIQxxz0L4S0_"
      },
      "source": [
        "tfidfVectorizer_sent = TfidfVectorizer()\n",
        "tfidfVectorizer_corpus = TfidfVectorizer()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94XvKsbU4uUQ"
      },
      "source": [
        "model_tfidfVectorizer_sent  = tfidfVectorizer_sent.fit(df['Word_segmentation'])\n",
        "model_tfidfVectorizer_corpus = tfidfVectorizer_corpus.fit_transform(df['Word_segmentation'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-pVc_N7LmR"
      },
      "source": [
        "binaryVectorizer_sent = CountVectorizer(binary=True)\n",
        "binaryVectorizer_corpus = CountVectorizer(binary=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxdR1w3_7WSY"
      },
      "source": [
        "model_binaryVectorizer_sent = binaryVectorizer_sent.fit(df['Word_segmentation'])\n",
        "model_binaryVectorizer_corpus = binaryVectorizer_corpus.fit_transform(df['Word_segmentation'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKqtLd2azssb"
      },
      "source": [
        "def ir_countVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_countVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_countVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAht7LyC5SnZ"
      },
      "source": [
        "def ir_tfidfVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_tfidfVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_tfidfVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW_weDb67tpo"
      },
      "source": [
        "def ir_tfidfVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_binaryVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_binaryVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1K3Rv5k7GHt"
      },
      "source": [
        "def ir_binary(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_tfidfVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_tfidfVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tA78qtF-3VX"
      },
      "source": [
        "w2v_model = Word2Vec(df['Tokenize_list'], size = 300, min_count = 2, workers = 3)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijkuPCKK-m81"
      },
      "source": [
        "embedding_each_sent = list()\n",
        "for sent_tokenize in df['Tokenize_list']:\n",
        "  embedding = list()\n",
        "  for word in sent_tokenize:\n",
        "    if word not in w2v_model.wv.vocab:\n",
        "      #300 dimension is the same as size of output embedding word2vec\n",
        "      embedding.append(np.random.rand(300))  \n",
        "    else:\n",
        "      embedding.append(w2v_model.wv.word_vec(word))\n",
        "  embedding = np.mean(embedding, axis = 0)\n",
        "  embedding_each_sent.append(embedding)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6IZh-3I-xuB"
      },
      "source": [
        "df['Gensim_embedding'] = embedding_each_sent"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0OJHz3oG4AC"
      },
      "source": [
        "def ir_gensim(question):\n",
        "  question = question.lower()\n",
        "  tokenize_q = word_tokenize(question)\n",
        "  q_embedding = list()\n",
        "  for word in tokenize_q:\n",
        "    if word not in w2v_model.wv.vocab:\n",
        "        #300 dimension is the same as size of output embedding word2vec\n",
        "      q_embedding.append(np.random.rand(300))  \n",
        "    else:\n",
        "      q_embedding.append(w2v_model.wv.word_vec(word))\n",
        "  q_embedding = np.mean(q_embedding, axis = 0)\n",
        "\n",
        "  r = list()\n",
        "  for embedding in df['Gensim_embedding']:\n",
        "    r.append(cosine_similarity(np.array([q_embedding]), np.array([embedding]))[0][0])\n",
        "    \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ernbif8YFiwg"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2fVjiZoB9Tq"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ENY3nfCAi1",
        "outputId": "bfe289c4-2d10-442f-e31c-8273525d7500"
      },
      "source": [
        "phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# For transformers v4.x+: \n",
        "phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXArmh0bPjr0"
      },
      "source": [
        "cos_bert = torch.nn.CosineSimilarity(dim=1, eps=1e-6)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOzJMWJVDpot"
      },
      "source": [
        "phobert_id_each_sent = [ phobert_tokenizer.encode(line) for line in df['Word_segmentation']]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG96Wod6ExfA"
      },
      "source": [
        "# convert list to torch tensor format\n",
        "phobert_id_each_sent =  [torch.tensor([sent]) for sent in phobert_id_each_sent]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNa9ZmSIZyb"
      },
      "source": [
        "def bert_embedding(phobert_id_each_sent):\n",
        "  r = list()\n",
        "  i = 0\n",
        "  for sent in phobert_id_each_sent:\n",
        "    # with torch.no_grad():\n",
        "    embedding = phobert_model(sent)[0][0]\n",
        "    r.append(np.mean(embedding.detach().numpy() , axis = 0))\n",
        "  return r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbwigPmEJnyw",
        "outputId": "596ab303-0306-4c42-cb96-04aadcbc93c6"
      },
      "source": [
        "phobert_model(phobert_id_each_sent[0])[0][0].detach().numpy()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.3426484e-01,  2.9970169e-01,  3.3295877e-02, ...,\n",
              "        -3.6920020e-01,  2.6532916e-02,  2.3787358e-01],\n",
              "       [-7.9690451e-03, -3.7074766e-01,  8.3288543e-02, ...,\n",
              "        -1.5157704e-01,  6.5741909e-04,  6.8149328e-01],\n",
              "       [ 9.0758212e-02,  4.4950882e-01, -1.5434603e-01, ...,\n",
              "        -2.6773065e-01, -2.4946736e-01,  5.5675024e-01],\n",
              "       ...,\n",
              "       [ 3.6425075e-01,  2.7860841e-01, -1.6791721e-01, ...,\n",
              "        -2.2941221e-01,  2.4586850e-01,  3.3459982e-01],\n",
              "       [ 1.2696093e-01,  3.8982850e-01, -1.9373862e-01, ...,\n",
              "        -5.1740015e-01,  1.2289950e-01,  6.0243094e-01],\n",
              "       [ 1.5069169e-01,  2.4493612e-01,  1.5941775e-01, ...,\n",
              "        -3.2291153e-01, -1.9354339e-01,  1.8636650e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH6mYlu3F5Vv"
      },
      "source": [
        "embedding_bert = bert_embedding(phobert_id_each_sent)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_1fXEvBOHUB",
        "outputId": "721a6871-39a5-4762-a536-5c926b7692a3"
      },
      "source": [
        "np.array(embedding_bert).shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5852, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBqYutefCEnv"
      },
      "source": [
        "df['Bert_embedding'] = embedding_bert"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "M8L4w1jAOjXd",
        "outputId": "ef8cf198-bd3e-455d-eead-fdabd68976ed"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Word_segmentation</th>\n",
              "      <th>Tokenize_list</th>\n",
              "      <th>Gensim_embedding</th>\n",
              "      <th>Bert_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim Marvel nhất</td>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>[bạn, thích, hãng, phim, nào, nhất, ?]</td>\n",
              "      <td>[-0.0881896, -0.016349202, -0.35377175, -0.160...</td>\n",
              "      <td>[0.09159158, 0.24511947, -0.0973822, -0.332693...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "      <td>thể_loại phim bạn thích là gì ?</td>\n",
              "      <td>[thể loại, phim, bạn, thích, là, gì, ?]</td>\n",
              "      <td>[-0.08587737, -0.016290462, -0.3477278, -0.155...</td>\n",
              "      <td>[-0.01886774, 0.19260995, -0.04660156, -0.5031...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ Noo Phước Thịnh</td>\n",
              "      <td>bạn có thần_tượng ca_sĩ nào không ?</td>\n",
              "      <td>[bạn, có, thần tượng, ca sĩ, nào, không, ?]</td>\n",
              "      <td>[-0.07320876, -0.0122848395, -0.2983003, -0.13...</td>\n",
              "      <td>[0.12758176, -0.017866714, -0.14962307, -0.553...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>[bạn, có, muốn, thử, vận, may, vào, việc, chơi...</td>\n",
              "      <td>[-0.022800383024037398, 0.09819823832119891, -...</td>\n",
              "      <td>[-0.1491277, 0.4655893, -0.0659392, -0.2527817...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>[tết, này, bạn, có, muốn, đi, xem, phim, không...</td>\n",
              "      <td>[-0.09164353, -0.014625112, -0.38200697, -0.17...</td>\n",
              "      <td>[-0.009907469, 0.09272019, 0.009779289, -0.287...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "      <td>cuối tuần này bạn có rãnh hay không ?</td>\n",
              "      <td>[cuối, tuần, này, bạn, có, rãnh, hay, không, ?]</td>\n",
              "      <td>[-0.08381217, -0.012829467, -0.3441187, -0.159...</td>\n",
              "      <td>[0.12912, 0.3531937, 0.090220764, -0.18896861,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>[bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...</td>\n",
              "      <td>[-0.08794381, -0.014473697, -0.3639611, -0.170...</td>\n",
              "      <td>[-0.0017565131, 0.19291644, -0.092288755, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi_mình ...</td>\n",
              "      <td>[mai, bạn, có, muốn, đi, đá, banh, chung, với,...</td>\n",
              "      <td>[-0.072087236, -0.011021971, -0.2984917, -0.13...</td>\n",
              "      <td>[-0.10028739, 0.28775898, -0.061872803, -0.266...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "      <td>bạn có dự_định đi chơi đâu vào cuối tuần không ?</td>\n",
              "      <td>[bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...</td>\n",
              "      <td>[-0.08064279, -0.012735165, -0.3323268, -0.154...</td>\n",
              "      <td>[-0.017464964, 0.10116356, 0.06738061, -0.2574...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể_loại gì ?</td>\n",
              "      <td>[bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...</td>\n",
              "      <td>[-0.08316428, -0.016183775, -0.3473207, -0.153...</td>\n",
              "      <td>[-0.08897535, 0.1993293, 0.13860983, -0.296447...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  ...                                     Bert_embedding\n",
              "0                    bạn thích hãng phim nào nhất ?   ...  [0.09159158, 0.24511947, -0.0973822, -0.332693...\n",
              "1                   thể loại phim bạn thích là gì ?   ...  [-0.01886774, 0.19260995, -0.04660156, -0.5031...\n",
              "2               bạn có thần tượng ca sĩ nào không ?   ...  [0.12758176, -0.017866714, -0.14962307, -0.553...\n",
              "3        bạn có muốn thử vận may vào việc chơi đề ?   ...  [-0.1491277, 0.4655893, -0.0659392, -0.2527817...\n",
              "4           tết này bạn có muốn đi xem phim không ?   ...  [-0.009907469, 0.09272019, 0.009779289, -0.287...\n",
              "5              cuối tuần này bạn có rãnh hay không?   ...  [0.12912, 0.3531937, 0.090220764, -0.18896861,...\n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...  ...  [-0.0017565131, 0.19291644, -0.092288755, -0.1...\n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...  ...  [-0.10028739, 0.28775898, -0.061872803, -0.266...\n",
              "8   bạn có dự định đi chơi đâu vào cuối tuần không?   ...  [-0.017464964, 0.10116356, 0.06738061, -0.2574...\n",
              "9  bạn cho mình hỏi bạn thích xem phim thể loại gì?   ...  [-0.08897535, 0.1993293, 0.13860983, -0.296447...\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PckmYbiEB5RY"
      },
      "source": [
        "def ir_bert(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_id = torch.tensor([phobert_tokenizer.encode(question)])\n",
        "  \n",
        "  q_embedding = phobert_model(q_id)[0][0]\n",
        "  q_embedding = np.mean(q_embedding.detach().numpy() , axis = 0)\n",
        "\n",
        "  r = list()\n",
        "  for vector in df['Bert_embedding']:\n",
        "    cos = cos_bert(torch.Tensor([q_embedding]), torch.Tensor([vector]))\n",
        "    cos = cos.detach().numpy()[0]\n",
        "    r.append(cos)\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVD35DQ3ivV"
      },
      "source": [
        "def ir():\n",
        "  while(True):\n",
        "    q = input(\"Nhập question: \")\n",
        "    if q == \"q\":\n",
        "      break\n",
        "\n",
        "    print(\"bot with count_vectorize: \", ir_countVectorizer(q))\n",
        "    print(\"bot with tfidf_vectorize: \", ir_tfidfVectorizer(q))\n",
        "    print(\"bot with binary_vectorize: \", ir_binary(q))\n",
        "    print(\"bot with gensim_embedding: \", ir_gensim(q))\n",
        "    print('bot with bert_embedding: ', ir_bert(q))\n",
        "    "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-_8_x234r8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6da713-48b9-426b-aed7-be9e46b04684"
      },
      "source": [
        "ir()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nhập question: chào bạn nha\n",
            "bot with count_vectorize:   Chào bạn, mình tên là Hùng \n",
            "bot with tfidf_vectorize:   Tôi ổn. còn bạn thì Sao? \n",
            "bot with binary_vectorize:   Chào bạn, mình tên là Hùng \n",
            "bot with gensim_embedding:   có \n",
            "bot with bert_embedding:   Không có gì đâu \n",
            "Nhập question: dạo này bạn có khỏe không thế?\n",
            "bot with count_vectorize:   mình có đi đà lạt hồi noel 2020 \n",
            "bot with tfidf_vectorize:   mình có đi đà lạt hồi noel 2020 \n",
            "bot with binary_vectorize:   Đang chán đời cậu ơi, muốn giải tỏa. \n",
            "bot with gensim_embedding:   không, mình chỉ có màu xanh thôi \n",
            "bot with bert_embedding:   Đang chán đời cậu ơi, muốn giải tỏa. \n",
            "Nhập question: q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}