{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_bai1.ipynb",
      "provenance": [],
      "mount_file_id": "1nn6Pz0N_zfbAy2p6iGuv5FBVUf9_qxzw",
      "authorship_tag": "ABX9TyPcDes4cZihUVrgDjV2JTq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1000Roses/NLP_course/blob/main/final_bai1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEJqdgdqX3TO",
        "outputId": "b20a0088-e70c-470e-f378-0cb9cba5e46a"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Requirement already satisfied: torch<=1.5.1,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.5.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.9.7)\n",
            "Requirement already satisfied: transformers<=3.5.1,>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.5.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.0.45)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (0.9.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2tWRDvbX8Du"
      },
      "source": [
        "from underthesea import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKx6HOoNJg0t",
        "outputId": "2d870b4a-e030-4cac-e0c6-bb63b91d42a1"
      },
      "source": [
        "import glob\n",
        "print(glob.glob(\"/content/drive/MyDrive/NLP/chatbotdata/*.txt\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/NLP/chatbotdata/giải trí.txt', '/content/drive/MyDrive/NLP/chatbotdata/du lịch.txt', '/content/drive/MyDrive/NLP/chatbotdata/shoping.txt', '/content/drive/MyDrive/NLP/chatbotdata/gia đình.txt', '/content/drive/MyDrive/NLP/chatbotdata/tán gẫu.txt', '/content/drive/MyDrive/NLP/chatbotdata/các câu hỏi phức tạp.txt', '/content/drive/MyDrive/NLP/chatbotdata/đất nước.txt', '/content/drive/MyDrive/NLP/chatbotdata/nghỉ lễ.txt', '/content/drive/MyDrive/NLP/chatbotdata/học tập.txt', '/content/drive/MyDrive/NLP/chatbotdata/thông tin cá nhân.txt', '/content/drive/MyDrive/NLP/chatbotdata/sở thích.txt', '/content/drive/MyDrive/NLP/chatbotdata/trò chuyện về đi ăn.txt', '/content/drive/MyDrive/NLP/chatbotdata/robot.txt', '/content/drive/MyDrive/NLP/chatbotdata/địa chỉ.txt', '/content/drive/MyDrive/NLP/chatbotdata/người yêu.txt', '/content/drive/MyDrive/NLP/chatbotdata/tdtu.txt', '/content/drive/MyDrive/NLP/chatbotdata/bạn bè.txt', '/content/drive/MyDrive/NLP/chatbotdata/nghề nghiệp.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk468adXJnYK"
      },
      "source": [
        "all_path = glob.glob(\"/content/drive/MyDrive/NLP/chatbotdata/*.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEDJHPAxX_y3"
      },
      "source": [
        "FRIEND_PRE_DATA = list()\n",
        "VOCAB_SIZE = 0\n",
        "# file = ['bạn bè', 'du lịch', 'gia đình', 'giải trí','học tập', 'nghề nghiệp', 'người yêu','sở thích','tán gẫu','tdtu', 'đất nước', 'địa chỉ','các câu hỏi phức tạp']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng3dd1wQYGP2"
      },
      "source": [
        "for path in all_path:\n",
        "  with open(path) as frienddata:\n",
        "    for data in frienddata:\n",
        "      sent = data.split(\"__eou__\")[:2]\n",
        "      if sent[0] == ' ' or sent[1] == ' ':\n",
        "        continue\n",
        "      if len(sent) == 2:\n",
        "        anq = list()\n",
        "        for subsent in sent:\n",
        "          anq.append(subsent)\n",
        "        FRIEND_PRE_DATA.append(anq)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Zse5mnZWZW"
      },
      "source": [
        "df = pd.DataFrame(FRIEND_PRE_DATA, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "jR9VZPFDZgyT",
        "outputId": "874e22d4-9a86-4838-d1b8-affb739a306b"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim Marvel nhất</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ Noo Phước Thịnh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                            Answer\n",
              "0                    bạn thích hãng phim nào nhất ?                  mình thích hãng phim Marvel nhất \n",
              "1                   thể loại phim bạn thích là gì ?    mình thích thể loại phim hành động, viễn tưởng \n",
              "2               bạn có thần tượng ca sĩ nào không ?              mình rất thích ca sĩ Noo Phước Thịnh \n",
              "3        bạn có muốn thử vận may vào việc chơi đề ?                            mình không muốn cờ bạc \n",
              "4           tết này bạn có muốn đi xem phim không ?                      tất nhiên rồi, mình rất muốn \n",
              "5              cuối tuần này bạn có rãnh hay không?                           cuối tuần này mình rãnh \n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...                             dĩ nhiên là muốn rồi \n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...                 xin lỗi mai mình có việc bận rồi \n",
              "8   bạn có dự định đi chơi đâu vào cuối tuần không?            không hiện tại mình chưa có dự định gì \n",
              "9  bạn cho mình hỏi bạn thích xem phim thể loại gì?              tớ thích xem phim thể loại hành động "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuHBeU9stma"
      },
      "source": [
        "document_token = [ \" \".join([ e.replace(' ','_') for e in word_tokenize(sent.lower())]) for sent in df['Question']]\n",
        "document_token_list = [word_tokenize(sent.lower()) for sent in df['Question']]  \n",
        "question_lower = [question.lower() for question in df['Question']]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VxGzbAhsuU5"
      },
      "source": [
        "df[\"Tokenize_string\"] = document_token\n",
        "df[\"Tokenize_list\"] = document_token_list\n",
        "df['Question'] = question_lower"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "3vBIXGRrsv8_",
        "outputId": "3f01de09-2774-4d2f-f189-ac2e89cdb6ea"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Tokenize_string</th>\n",
              "      <th>Tokenize_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim Marvel nhất</td>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>[bạn, thích, hãng, phim, nào, nhất, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "      <td>thể_loại phim bạn thích là gì ?</td>\n",
              "      <td>[thể loại, phim, bạn, thích, là, gì, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ Noo Phước Thịnh</td>\n",
              "      <td>bạn có thần_tượng ca_sĩ nào không ?</td>\n",
              "      <td>[bạn, có, thần tượng, ca sĩ, nào, không, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>[bạn, có, muốn, thử, vận, may, vào, việc, chơi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>[tết, này, bạn, có, muốn, đi, xem, phim, không...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "      <td>cuối tuần này bạn có rãnh hay không ?</td>\n",
              "      <td>[cuối, tuần, này, bạn, có, rãnh, hay, không, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>[bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi_mình ...</td>\n",
              "      <td>[mai, bạn, có, muốn, đi, đá, banh, chung, với,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "      <td>bạn có dự_định đi chơi đâu vào cuối tuần không ?</td>\n",
              "      <td>[bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể_loại gì ?</td>\n",
              "      <td>[bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  ...                                      Tokenize_list\n",
              "0                    bạn thích hãng phim nào nhất ?   ...             [bạn, thích, hãng, phim, nào, nhất, ?]\n",
              "1                   thể loại phim bạn thích là gì ?   ...            [thể loại, phim, bạn, thích, là, gì, ?]\n",
              "2               bạn có thần tượng ca sĩ nào không ?   ...        [bạn, có, thần tượng, ca sĩ, nào, không, ?]\n",
              "3        bạn có muốn thử vận may vào việc chơi đề ?   ...  [bạn, có, muốn, thử, vận, may, vào, việc, chơi...\n",
              "4           tết này bạn có muốn đi xem phim không ?   ...  [tết, này, bạn, có, muốn, đi, xem, phim, không...\n",
              "5              cuối tuần này bạn có rãnh hay không?   ...    [cuối, tuần, này, bạn, có, rãnh, hay, không, ?]\n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...  ...  [bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...\n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...  ...  [mai, bạn, có, muốn, đi, đá, banh, chung, với,...\n",
              "8   bạn có dự định đi chơi đâu vào cuối tuần không?   ...  [bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...\n",
              "9  bạn cho mình hỏi bạn thích xem phim thể loại gì?   ...  [bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZfhVas-bOPU"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p0LQ2QUsTpV"
      },
      "source": [
        "countVectorizer_sent = CountVectorizer()\n",
        "countVectorizer_corpus = CountVectorizer()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIiYa8kxa9nT"
      },
      "source": [
        "model_countVectorizer_sent = countVectorizer_sent.fit(df['Tokenize_string'])\n",
        "model_countVectorizer_corpus = countVectorizer_sent.fit_transform(df['Tokenize_string'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIQxxz0L4S0_"
      },
      "source": [
        "tfidfVectorizer_sent = TfidfVectorizer()\n",
        "tfidfVectorizer_corpus = TfidfVectorizer()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94XvKsbU4uUQ"
      },
      "source": [
        "model_tfidfVectorizer_sent  = tfidfVectorizer_sent.fit(df['Tokenize_string'])\n",
        "model_tfidfVectorizer_corpus = tfidfVectorizer_corpus.fit_transform(df['Tokenize_string'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-pVc_N7LmR"
      },
      "source": [
        "binaryVectorizer_sent = CountVectorizer(binary=True)\n",
        "binaryVectorizer_corpus = CountVectorizer(binary=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxdR1w3_7WSY"
      },
      "source": [
        "model_binaryVectorizer_sent = binaryVectorizer_sent.fit(df['Tokenize_string'])\n",
        "model_binaryVectorizer_corpus = binaryVectorizer_corpus.fit_transform(df['Tokenize_string'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKqtLd2azssb"
      },
      "source": [
        "def ir_countVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_countVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_countVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAht7LyC5SnZ"
      },
      "source": [
        "def ir_tfidfVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_tfidfVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_tfidfVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW_weDb67tpo"
      },
      "source": [
        "def ir_tfidfVectorizer(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_binaryVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_binaryVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1K3Rv5k7GHt"
      },
      "source": [
        "def ir_binary(question):\n",
        "  # Tokenize\n",
        "  question = \" \".join([ token.replace(\" \", \"_\")  for token in word_tokenize(question.lower())])\n",
        "  q_vector = model_tfidfVectorizer_sent.transform([question]).toarray()[0]\n",
        "\n",
        "  r = list()\n",
        "  for vector in model_tfidfVectorizer_corpus.toarray():\n",
        "    r.append(cosine_similarity(np.array([q_vector]), np.array([vector]))[0][0])\n",
        "  \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tA78qtF-3VX"
      },
      "source": [
        "w2v_model = Word2Vec(df['Tokenize_list'], size = 300, min_count = 2, workers = 3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijkuPCKK-m81"
      },
      "source": [
        "embedding_each_sent = list()\n",
        "for sent_tokenize in df['Tokenize_list']:\n",
        "  embedding = list()\n",
        "  for word in sent_tokenize:\n",
        "    if word not in w2v_model.wv.vocab:\n",
        "      #300 dimension is the same as size of output embedding word2vec\n",
        "      embedding.append(np.random.rand(300))  \n",
        "    else:\n",
        "      embedding.append(w2v_model.wv.word_vec(word))\n",
        "  embedding = np.mean(embedding, axis = 0)\n",
        "  embedding_each_sent.append(embedding)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6IZh-3I-xuB"
      },
      "source": [
        "df['Embedding_gensim'] = embedding_each_sent"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0OJHz3oG4AC"
      },
      "source": [
        "def ir_gensim(question):\n",
        "  question = question.lower()\n",
        "  tokenize_q = word_tokenize(question)\n",
        "  q_embedding = list()\n",
        "  for word in tokenize_q:\n",
        "    if word not in w2v_model.wv.vocab:\n",
        "        #300 dimension is the same as size of output embedding word2vec\n",
        "      q_embedding.append(np.random.rand(300))  \n",
        "    else:\n",
        "      q_embedding.append(w2v_model.wv.word_vec(word))\n",
        "  q_embedding = np.mean(q_embedding, axis = 0)\n",
        "\n",
        "  r = list()\n",
        "  for embedding in df['Embedding_gensim']:\n",
        "    r.append(cosine_similarity(np.array([q_embedding]), np.array([embedding]))[0][0])\n",
        "    \n",
        "  r = np.array(r)\n",
        "  i = np.argmax(r)\n",
        "  return df['Answer'][i]\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVD35DQ3ivV"
      },
      "source": [
        "def ir():\n",
        "  while(True):\n",
        "    q = input(\"Nhập question: \")\n",
        "    if q == \"q\":\n",
        "      break\n",
        "  \n",
        "    print(\"bot with count_vectorize: \", ir_countVectorizer(q))\n",
        "    print(\"bot with tfidf_vectorize: \", ir_tfidfVectorizer(q))\n",
        "    print(\"bot with binary_vectorize: \", ir_binary(q))\n",
        "    print(\"bot with gensim_embedding: \", ir_gensim(q))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-_8_x234r8",
        "outputId": "ab16547b-7a8e-4d59-f8a5-a1fc5391903f"
      },
      "source": [
        "ir()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nhập question: bạn có crush ai không?\n",
            "bot with count_vectorize:   Mình có thích một người ^^ \n",
            "bot with tfidf_vectorize:   có, mình có crush một người. \n",
            "bot with binary_vectorize:   Mình có thích một người ^^ \n",
            "bot with gensim_embedding:   Mình có thích một người ^^ \n",
            "Nhập question: okay bạn\n",
            "bot with count_vectorize:   Đoán xem \n",
            "bot with tfidf_vectorize:   Mình ở quận 9 thành phố Hồ Chí Minh, ngay chỗ đại học sư phạm kỹ thuật á. \n",
            "bot with binary_vectorize:   ở đó có chỗ nào để đi chơi không \n",
            "bot with gensim_embedding:   không có gì \n",
            "Nhập question: bạn là một người rất vui tính\n",
            "bot with count_vectorize:   mình thấy bạn của mình toàn những đứa điên điên khùng khùng \n",
            "bot with tfidf_vectorize:   Thực sự thì mình đúng là người lười vận động \n",
            "bot with binary_vectorize:   mình thấy bạn của mình toàn những đứa điên điên khùng khùng \n",
            "bot with gensim_embedding:   Quan điểm của mình là không biết \n",
            "Nhập question: bạn có hâm mộ ai không?\n",
            "bot with count_vectorize:   Không có luôn. \n",
            "bot with tfidf_vectorize:   Không có luôn. \n",
            "bot with binary_vectorize:   Không có luôn. \n",
            "bot with gensim_embedding:   Không có luôn. \n",
            "Nhập question: bạn ơi\n",
            "bot with count_vectorize:   mình nghe nè bạn ơi \n",
            "bot with tfidf_vectorize:   mình nghe nè bạn ơi \n",
            "bot with binary_vectorize:   Chắc chứ \n",
            "bot with gensim_embedding:   được chứ bạn, bạn muốn hỏi gì cứ hỏi đi \n",
            "Nhập question: q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}