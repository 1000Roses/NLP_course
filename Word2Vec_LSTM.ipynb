{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_LSTM.ipynb",
      "provenance": [],
      "mount_file_id": "16dllZu5919MhlrJBn4d07p1dcJ41OpqA",
      "authorship_tag": "ABX9TyNa4RoQUKhH4EMQ3bm2uRTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1000Roses/NLP_course/blob/main/Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yZAtR2ObcJD"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install underthesea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoouNMzabfjM"
      },
      "source": [
        "from underthesea import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DmfbZwFbilp"
      },
      "source": [
        "import glob\n",
        "all_path = glob.glob(\"/content/drive/MyDrive/NLP/chatbotdata/*.txt\")"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDDD07blbmDg"
      },
      "source": [
        "FRIEND_PRE_DATA = list()\n",
        "VOCAB_SIZE = 0\n",
        "ALL_WORDS = ['<sos>', '<eos>', ' ']"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLorWilNbsYu"
      },
      "source": [
        "for path in all_path:\n",
        "  with open(path) as frienddata:\n",
        "    for data in frienddata:\n",
        "      sent = data.split(\"__eou__\")[:2]\n",
        "      if sent[0] == ' ' or sent[1] == ' ':\n",
        "        continue\n",
        "      if len(sent) == 2:\n",
        "        anq = list()\n",
        "        for subsent in sent:\n",
        "          # Remove leading, ending space in string, \" để mùa hạ qua từng đêm vắng  \"  -> \"để mùa hạ qua từng đêm vắng\"\n",
        "          subsent = subsent.strip()\n",
        "          anq.append(subsent)\n",
        "        FRIEND_PRE_DATA.append(anq)\n",
        "  break"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa73fBwbbumJ"
      },
      "source": [
        "df = pd.DataFrame(FRIEND_PRE_DATA, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOcIBDlKby4Y"
      },
      "source": [
        "questions_lower = [sent.lower() for sent in df['Question']]\n",
        "df['Question'] = questions_lower\n",
        "\n",
        "answer_lower = [sent.lower() for sent in df['Answer']]\n",
        "df['Answer'] = answer_lower"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJjjPorh_9w"
      },
      "source": [
        "question_tokenize = [word_tokenize(sent) for sent in df['Question']]\n",
        "answer_tokenize = [ ['<sos>'] + word_tokenize(sent) + ['<eos>'] for sent in df['Answer']]\n",
        "\n",
        "df['Question_tokenize']= question_tokenize\n",
        "df['Answer_tokenize']= answer_tokenize"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "ehJ06F8jb2FR",
        "outputId": "b8e1a7ef-b2c4-4c30-9720-eb8f9d616d52"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Question_tokenize</th>\n",
              "      <th>Answer_tokenize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bạn thích hãng phim nào nhất ?</td>\n",
              "      <td>mình thích hãng phim marvel nhất</td>\n",
              "      <td>[bạn, thích, hãng, phim, nào, nhất, ?]</td>\n",
              "      <td>[&lt;sos&gt;, mình, thích, hãng, phim, marvel, nhất,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thể loại phim bạn thích là gì ?</td>\n",
              "      <td>mình thích thể loại phim hành động, viễn tưởng</td>\n",
              "      <td>[thể loại, phim, bạn, thích, là, gì, ?]</td>\n",
              "      <td>[&lt;sos&gt;, mình, thích, thể loại, phim, hành động...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bạn có thần tượng ca sĩ nào không ?</td>\n",
              "      <td>mình rất thích ca sĩ noo phước thịnh</td>\n",
              "      <td>[bạn, có, thần tượng, ca sĩ, nào, không, ?]</td>\n",
              "      <td>[&lt;sos&gt;, mình, rất, thích, ca sĩ, noo, phước, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bạn có muốn thử vận may vào việc chơi đề ?</td>\n",
              "      <td>mình không muốn cờ bạc</td>\n",
              "      <td>[bạn, có, muốn, thử, vận, may, vào, việc, chơi...</td>\n",
              "      <td>[&lt;sos&gt;, mình, không, muốn, cờ bạc, &lt;eos&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tết này bạn có muốn đi xem phim không ?</td>\n",
              "      <td>tất nhiên rồi, mình rất muốn</td>\n",
              "      <td>[tết, này, bạn, có, muốn, đi, xem, phim, không...</td>\n",
              "      <td>[&lt;sos&gt;, tất nhiên, rồi, ,, mình, rất, muốn, &lt;e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cuối tuần này bạn có rãnh hay không?</td>\n",
              "      <td>cuối tuần này mình rãnh</td>\n",
              "      <td>[cuối, tuần, này, bạn, có, rãnh, hay, không, ?]</td>\n",
              "      <td>[&lt;sos&gt;, cuối, tuần, này, mình, rãnh, &lt;eos&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bạn có muốn đi xem phim vào cuối tuần này hay ...</td>\n",
              "      <td>dĩ nhiên là muốn rồi</td>\n",
              "      <td>[bạn, có, muốn, đi, xem, phim, vào, cuối, tuần...</td>\n",
              "      <td>[&lt;sos&gt;, dĩ nhiên, là, muốn, rồi, &lt;eos&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mai bạn có muốn đi đá banh chung với tụi mình ...</td>\n",
              "      <td>xin lỗi mai mình có việc bận rồi</td>\n",
              "      <td>[mai, bạn, có, muốn, đi, đá, banh, chung, với,...</td>\n",
              "      <td>[&lt;sos&gt;, xin lỗi, mai, mình, có, việc, bận, rồi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bạn có dự định đi chơi đâu vào cuối tuần không?</td>\n",
              "      <td>không hiện tại mình chưa có dự định gì</td>\n",
              "      <td>[bạn, có, dự định, đi, chơi, đâu, vào, cuối, t...</td>\n",
              "      <td>[&lt;sos&gt;, không, hiện tại, mình, chưa, có, dự đị...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bạn cho mình hỏi bạn thích xem phim thể loại gì?</td>\n",
              "      <td>tớ thích xem phim thể loại hành động</td>\n",
              "      <td>[bạn, cho, mình, hỏi, bạn, thích, xem, phim, t...</td>\n",
              "      <td>[&lt;sos&gt;, tớ, thích, xem, phim, thể loại, hành đ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  ...                                    Answer_tokenize\n",
              "0                     bạn thích hãng phim nào nhất ?  ...  [<sos>, mình, thích, hãng, phim, marvel, nhất,...\n",
              "1                    thể loại phim bạn thích là gì ?  ...  [<sos>, mình, thích, thể loại, phim, hành động...\n",
              "2                bạn có thần tượng ca sĩ nào không ?  ...  [<sos>, mình, rất, thích, ca sĩ, noo, phước, t...\n",
              "3         bạn có muốn thử vận may vào việc chơi đề ?  ...          [<sos>, mình, không, muốn, cờ bạc, <eos>]\n",
              "4            tết này bạn có muốn đi xem phim không ?  ...  [<sos>, tất nhiên, rồi, ,, mình, rất, muốn, <e...\n",
              "5               cuối tuần này bạn có rãnh hay không?  ...        [<sos>, cuối, tuần, này, mình, rãnh, <eos>]\n",
              "6  bạn có muốn đi xem phim vào cuối tuần này hay ...  ...            [<sos>, dĩ nhiên, là, muốn, rồi, <eos>]\n",
              "7  mai bạn có muốn đi đá banh chung với tụi mình ...  ...  [<sos>, xin lỗi, mai, mình, có, việc, bận, rồi...\n",
              "8    bạn có dự định đi chơi đâu vào cuối tuần không?  ...  [<sos>, không, hiện tại, mình, chưa, có, dự đị...\n",
              "9   bạn cho mình hỏi bạn thích xem phim thể loại gì?  ...  [<sos>, tớ, thích, xem, phim, thể loại, hành đ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwFgzt8fr0r6"
      },
      "source": [
        "# word2index, index2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CeneJPZwWc7"
      },
      "source": [
        "for question_sent_list, answer_sent_list in zip(df['Question_tokenize'], df['Answer_tokenize']):\n",
        "  for word in question_sent_list:\n",
        "    if word not in ALL_WORDS:\n",
        "      ALL_WORDS.append(word)\n",
        "  for word in answer_sent_list:\n",
        "    if word not in ALL_WORDS:\n",
        "      ALL_WORDS.append(word)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX3-fpfndwYl"
      },
      "source": [
        "ALL_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67BJ_IJVewKv"
      },
      "source": [
        "word2index= {word: i for i, word in enumerate(ALL_WORDS)}"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BRHM0Kue98M"
      },
      "source": [
        "index2word= {i: word for i, word in enumerate(ALL_WORDS)}"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4knupTMkih76"
      },
      "source": [
        "# Build w2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9fGTk5hVID"
      },
      "source": [
        "w2v_model = Word2Vec(df['Question_tokenize'] + df['Answer_tokenize'], size = 300, min_count = 1, workers = 3)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gApT8jbailrm",
        "outputId": "35057a77-b9f5-4781-9a61-0ad1d5ae9c50"
      },
      "source": [
        "w2v_model.wv.word_vec('<eos>').shape"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKlY0MFZr8fh"
      },
      "source": [
        "max_sequence_question = max(df['Question'], key = lambda x : len(x))\n",
        "max_len_question = len(max_sequence_question.split())"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyG0R90lfbg3"
      },
      "source": [
        "max_encoder_seq_length = max_len_question\n",
        "max_decoder_seq_length = 15\n",
        "num_decoder_tokens = num_encoder_tokens = 300 "
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-hL6cOVfLnl"
      },
      "source": [
        "# 3D array of shape (num_pairs, max_sentence_length_word, num_word)\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(df['Question']), max_encoder_seq_length, num_encoder_tokens), dtype= 'float32'\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(df['Question']), max_decoder_seq_length, num_decoder_tokens), dtype= 'float32'\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(df['Question']), max_decoder_seq_length, len(ALL_WORDS)), dtype= 'float32'\n",
        ")"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_XTu0H6rJLn",
        "outputId": "d60ee603-e48e-486d-b4c8-669ffc2671a7"
      },
      "source": [
        "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((271, 29, 300), (271, 15, 300), (271, 15, 726))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkTcNDISjOey"
      },
      "source": [
        "for i, (question_sent_list, answer_sent_list) in enumerate(zip(df['Question_tokenize'], df['Answer_tokenize'])):\n",
        "  for t, word in enumerate(question_sent_list):\n",
        "    try:\n",
        "      if word not in w2v_model.wv.vocab:\n",
        "        encoder_input_data[i, t] = np.random.rand(300)\n",
        "      else:\n",
        "        encoder_input_data[i, t] = w2v_model.wv.word_vec(word)\n",
        "    except:\n",
        "      break\n",
        "  encoder_input_data[i, t + 1:]= w2v_model.wv.word_vec('<eos>')\n",
        "\n",
        "  for t, word in enumerate(answer_sent_list):\n",
        "\n",
        "    if t == max_decoder_seq_length:\n",
        "      break\n",
        "\n",
        "    if word not in w2v_model.wv.vocab:\n",
        "      decoder_input_data[i, t] = np.random.rand(300)\n",
        "    else:\n",
        "      decoder_input_data[i, t] = w2v_model.wv.word_vec(word)\n",
        "    if t > 0:\n",
        "        decoder_target_data[i, t - 1, word2index[word]] = 1\n",
        "  decoder_input_data[i, t + 1: ]= w2v_model.wv.word_vec('<eos>')\n",
        "  decoder_target_data[i, t:, word2index['<eos>']] = 1 \n",
        "\n"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIDnuhwIgd65",
        "outputId": "86e21911-f9f7-4b43-849f-7f42cce30dcf"
      },
      "source": [
        "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((271, 29, 300), (271, 15, 300), (271, 15, 726))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5msPT2q8qdxi",
        "outputId": "f3668b8b-7a5c-42db-d252-16699ca9f59b"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "\n",
        "latent_dim = 400\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, 300))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c, *_ = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, 300))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(ALL_WORDS), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, 400), (None, 1121600     input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   [(None, None, 400),  1121600     input_14[0][0]                   \n",
            "                                                                 lstm_6[0][1]                     \n",
            "                                                                 lstm_6[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, None, 726)    291126      lstm_7[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,534,326\n",
            "Trainable params: 2,534,326\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOJW1PEoxG4T"
      },
      "source": [
        "batch_size = 135\n",
        "epochs = 3000\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics= ['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          shuffle=True,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU0Q7cmDqxH2"
      },
      "source": [
        "# inference setup\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kKZMNmWejZU"
      },
      "source": [
        "# save model\n",
        "encoder_model.save('models/3000epoch_encoder_model.h5')\n",
        "decoder_model.save('models/3000epoch_decoder_model.h5')\n",
        "model.save('models/3000epoch_model.h5')"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutk9jfWyQxP"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0]= w2v_model.wv.word_vec('<sos>')\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        # print(output_tokens.shape) # 1 1 112\n",
        "        word_index = np.argmax(output_tokens[0, -1, :])\n",
        "        word = index2word[word_index]\n",
        "        # print(sampled_char)\n",
        "        \n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (word == '<eos>' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "            break\n",
        "        decoded_sentence += word + \" \"\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0] = w2v_model.wv.word_vec(word)\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4534hecXCzxC",
        "outputId": "ced11bc9-685d-45f2-9a04-71c667e27aeb"
      },
      "source": [
        "df['Question'][:15]"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                        bạn thích hãng phim nào nhất ?\n",
              "1                       thể loại phim bạn thích là gì ?\n",
              "2                   bạn có thần tượng ca sĩ nào không ?\n",
              "3            bạn có muốn thử vận may vào việc chơi đề ?\n",
              "4               tết này bạn có muốn đi xem phim không ?\n",
              "5                  cuối tuần này bạn có rãnh hay không?\n",
              "6     bạn có muốn đi xem phim vào cuối tuần này hay ...\n",
              "7     mai bạn có muốn đi đá banh chung với tụi mình ...\n",
              "8       bạn có dự định đi chơi đâu vào cuối tuần không?\n",
              "9      bạn cho mình hỏi bạn thích xem phim thể loại gì?\n",
              "10     bạn cho mình hỏi bạn có hay đi chơi ở đâu không?\n",
              "11         bạn cho mình hỏi bạn thích xem phim gì nhất?\n",
              "12    bạn cho mình hỏi bạn thường đi xem phim chung ...\n",
              "13    bạn cho mình hỏi bạn có thích xem phim kinh dị...\n",
              "14    bạn cho mình hỏi bạn có thích ăn gì trong khi ...\n",
              "Name: Question, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqX-UR7C2KOl"
      },
      "source": [
        "for i in range(150):\n",
        "  print(decode_sequence(np.array([encoder_input_data[i]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YPvmFNUV1mQ"
      },
      "source": [
        "def word2vec_LSTM_bot():\n",
        "  while(True):\n",
        "    q = input(\"Your question: \")\n",
        "    if q == \"q\":\n",
        "      break\n",
        "    \n",
        "    q = word_tokenize(q)\n",
        "    q_encoder_input_data = np.zeros(\n",
        "      (1, max_encoder_seq_length, num_encoder_tokens), dtype= 'float32'\n",
        "    )\n",
        "\n",
        "    for i,word in enumerate(q):\n",
        "      if word not in w2v_model.wv.vocab:\n",
        "        q_encoder_input_data[0][i] = np.random.rand(300)\n",
        "      else:\n",
        "        q_encoder_input_data[0][i]= w2v_model.wv.word_vec(word) \n",
        "    \n",
        "\n",
        "    print(decode_sequence(q_encoder_input_data))"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w42QuieyXO2L",
        "outputId": "6d4f0cf3-4a32-4b41-861f-d818bbdbbc7d"
      },
      "source": [
        "word2vec_LSTM_bot()"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your question: bạn có crush chưa\n",
            "mình chọn nhà xe thành bưởi \n",
            "Your question: bạn đi xem phim bao giờ chưa\n",
            "mình cũng đi khá thường xuyên . \n",
            "Your question: bạn thấy phim nào hay nhất\n",
            "mình thích đi thảo cầm viên , \n",
            "Your question: bạn thích ai\n",
            "mình đang ăng rang rang bơ \n",
            "Your question: ok\n",
            "nếu bị mất thẻ sinh viên thì cậu phải liên hệ phòng \n",
            "Your question: q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}